---
layout:     post
title:      "经典网络结构"
data: 2019-05-08 12:34:56
permalink:  backbone.html
categories: 深度学习
tags: backbone
excerpt: 深度学习一些经典的网络框架的整理
mathjax: true
---

* content
{:toc}

## LeNet -- 1998
网络结构如下所示：

![lenet](../img/lenet.png)

特点：
* 引入了卷积层
* 引入了池化层(平均池化)
* 非线性激活函数（tanh、sigmoid）

## AlexNet -- 2012
![alexnet](../img/alexnet.png)

特点：
* 使用了ReLU
* 重叠的最大池化
* 使用了dorpout，数据增强
* 使用了多GPU
* 使用了LRN归一化层（激活的神经元抑制相邻神经元）

## ZFNet -- 2013
![zfnet](../img/zfnet.png)

特点：
* 使用反卷积（Deconvnet）可视化特征图
* 使用了更小的卷积核和更小的步长


## NiN -- 2014
![nin](../img/nin.png)

特点：
* 网络中间插入网络
* 提出全局平均池化（Global Average Pooling）代替全连接层
* 使用了$1*1$卷积层
* 使用了MaxOut
  
## GoogLeNet(Inception V1) -- 2014
Inception基本结构如下：

![inception](../img/inception.png)

由于参数过多，借鉴NiN中$1 * 1$卷积后，改进为：

![inception2](../img/inception2.png)

## VGGNet -- 2014
网络结构如下：

![vggnet](../img/vggnet.png)

特点：
* 证明了LRN没用
* 证明了增加深度能提高性能
* 只用$3\*3$卷积和$2\*2$池化

## ResNet -- 2015
ResNet最初的结构如图：

![resnet](../img/resnet.png)

ResNet发展的各种变体：

![resnet2](../img/resnet2.png)

## Inception V2和V3 -- 2015
v2: 将$5\*5$卷积换成两个$3\*3$卷积，引入BN层 

v3: 将$n\*n$卷积换为$1\*n$和$n\*1$卷积

![inceptionv2](../img/inceptionv2.png)

## Inception V4 -- 2016
使用了不同的Inception块，具体如下：

![inceptionv4](../img/inceptionv4.png)

整体网络结构如下：

![inceptionv41](../img/inceptionv41.png)

## Inception-ResNet v1
结合了ResNet，结构如下：

![inceptionresnetv1](../img/inceptionresnetv1.png)

inception-resnet C模块如下：

![inceptionresnetv11](../img/inceptionresnetv11.png)

## Inception-ResNet v2
inception结构如下：

![inceptionresnetv2](../img/inceptionresnetv2.png)

Inception-ResNet v1和v2整体网络结构如下：

![inceptionresnetv21](../img/inceptionresnetv21.png)

作者们实验发现如果对inception-resnet网络中的residual模块的输出进行scaling（如以0.1-0.3），那么可以让它的整个训练过程更加地稳定。如下图为scaling的具体做法示意。

![inveptionresnetv22](../img/inceptionresnetv22.png)

## SqueezeNet -- 2016
SqueezeNet是一种轻量级网络，参数比AlexNet少50x，但模型性能与AlexNet接近。

主要设计思想：
1. 替换部分3×3的卷积为1×1卷积
2. 减少输入3×3卷积的特征图数目（具体的通过设计Fire模块）
3. 延迟下采样可以提升模型准确度（下采样方式一般为strides>1的卷积层或者池化层）

Fire模块结构如下：

![squeezenet](../img/squeezenet.png)

具体网络及引入了resnet的网络结构如下：

![squeezenet1](../img/squeezenet1.png)

## Xception -- 2017
Xception模块如下：

![xception](../img/xception.png)

Xception与原版的Depth-wise convolution有两个不同之处：

1. 原版Depth-wise convolution先逐通道卷积，再1×1卷积;而Xception是反过来，先 1×1卷积，再逐通道卷积； 
2. 原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1×1卷积之后会带上一个Relu的非线性激活函数；

整体网络结构如下：

![xception1](../img/xception1.png)

## ShuffleNet -- 2017
核心特点：
1. channel shuffle
2. pointwise group convolutions
3. depthwise separable convolution

group convolutions和channel shuffle如下图所示：

![shuffleNet](../img/shufflenet.png)

pointwise group convolutions其实就是卷积核为1×1的group convolutions，depthwise separable convolution和MobileNet里面的一样。最终，修改后的ShuffleNet模块如下所示：

![shufflenet2](../img/shufflenet2.png)


## DenseNet -- 2017
和resnet类似，每个DenseNet块如下图所示：

![densenet](../img/densenet.png)

## SENet -- 2017
SENet模块如下所示：

![senet](../img/senet.png)

图中的$F_{tr}$是传统的卷积结构，X和U是$F_{tr}$的输入（C'xH'xW'）和输出（CxHxW），这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（图中的$F_{sq}(\cdot)$，作者称为Squeeze过程），输出的1x1xC数据再经过两级全连接（图中的$F_{ex}(\cdot)$，作者称为Excitation过程），最后用sigmoid（论文中的self-gating mechanism）限制到[0，1]的范围，把这个值作为scale乘到U的C个通道上， 作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。

## MobileNet V1 -- 2017
主要提出了depthwise separable convolution（深度可分离卷积），主要分两部分：depthwise convolution和pointwise convolution，Depthwise convolution和标准卷积不同，对于标准卷积其卷积核是用在所有的输入通道上（input channels），而depthwise convolution针对每个输入通道采用不同的卷积核，就是说一个卷积核对应一个输入通道，所以说depthwise convolution是depth级别的操作。而pointwise convolution其实就是普通的卷积，只不过其采用1x1的卷积核。示意图如下：

![mobilenet](../img/mobilenet.png)

![mobilenet1](../img/mobilenet1.png)

## MobileNet V2 -- 2018
主要结合了MobileNet V1和Shortcut connection（ResNet、DenseNet）的思想。

## ResNeXt
网络模块的几种等价形式如下：

![resnext](../img/resnext.png)

ResNext和Inception区别为：ResNext为相加，而Inception为级联。

## Res2Net
Res2Net模块如下：

![res2net](../img/res2net.png)

其中3×3的卷积可以替换成其他模块

![res2net1](../img/res2net1.png)


## MobileNet V3 -- 2019
用了神经架构搜索，没有引入新的block，block继承自：
1. MobileNetV1中的deepthwisse separable convolutions（深度可分离卷积）。
2. MobileNetV2中的具有线性瓶颈的倒残差结构
3. MnasNet（NasNet、MnasNet这种实在学不动啦～）中引入的基于squeeze and excitation结构的轻量级注意力模型